\documentclass[12pt]{cctart}
\vsize=29.7 true cm \hsize=21.0 true cm \topmargin 0pt
\oddsidemargin 12pt \evensidemargin 12pt \textheight=22.5 true cm
\textwidth=15.5 true cm \pagestyle{plain}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\DeclareMathOperator*{\argmax}{argmax}

\vfuzz2pt
\hfuzz2pt

\def \[{\begin{equation}}
\def \]{\end{equation}}
\def \l{\left}
\def \r{\right}
\def \endzm{\hfill$\sqcap \!\!\!\! \sqcup$\bigskip}
\renewcommand{\baselinestretch}{2.4}


\usepackage{threeparttable}

\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{tocloft}
\let\CCTCJKfonts=1
\input CCT.sty
\renewcommand{\cftsecdotsep}{2}
\renewcommand{\cftsubsecdotsep}{2}
\renewcommand{\cfttoctitlefont}{\LARGE\heiti\hfill}
\renewcommand{\cftaftertoctitle}{\\ [0.3\baselineskip]\mbox{}\hfill{\Large \bf Contents}}
\renewcommand{\cftaftertoctitleskip}{40mm}

\numberwithin{equation}{section}

\newtheorem{theorem}{定理}[section]
\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
\newtheorem{definition}{定义}[section]
\renewcommand{\thedefinition}{\arabic{section}.\arabic{definition}}
\newtheorem{proposition}{Proposition}[section]
\renewcommand{\theproposition}{\arabic{section}.\arabic{proposition}}
\newtheorem{corollary}{推论}[section]
\renewcommand{\thecorollary}{\arabic{section}.\arabic{corollary}}
\newtheorem{lemma}{引理}[section]
\renewcommand{\thelemma}{\arabic{section}.\arabic{lemma}}
\newtheorem{remark}{注}
\renewcommand{\theremark}{\arabic{section}.\arabic{remark}}
\newtheorem{proof}{证明}
\renewcommand{\theproof}{}
\newtheorem{example}{例}[section]
\renewcommand{\theexample}{\arabic{section}.\arabic{example}}
\newtheorem{hyper}{假设}[section]
\renewcommand{\thehyper}{\arabic{section}.\arabic{hyper}}

%%%%%%%%%%%%%%%%%%From DU YU %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\rem}{{\it Remark }}

\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\norml}[2]{\left\Vert#1\right\Vert_{L^2(#2)}}
\newcommand{\norme}[1]{\left\Vert{\hskip -2.7pt}\left\vert #1 \right\vert{\hskip -2.7pt}\right\Vert}
\newcommand{\normes}[1]{\left\Vert{\hskip -1pt}\left\vert #1 \right\vert{\hskip -1pt}\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\pd}[1]{\left\langle #1\right\rangle}
\newcommand{\pdjj}[2]{\left\langle \left[#1\right], \left[#2\right]\right\rangle_e}
\newcommand{\pdaj}[2]{\left\langle \left\{#1\right\}, \left[#2\right]\right\rangle_e}
\newcommand{\pdja}[2]{\left\langle \left[#1\right], \left\{#2\right\}\right\rangle_e}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\av}[1]{\left\{#1\right\}}
\newcommand{\jm}[1]{\left[#1\right]}
\newcommand{\nfrac}[2]{\displaystyle{\genfrac{}{}{0pt}{}{#1}{#2}}}

\newcommand{\wcT}{\widehat{\mathcal{T}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tuh}{\tilde{u}_h}
\newcommand{\csta}{C_{\rm sta}}
\newcommand{\cerr}{C_{\rm err}}

\newcommand{\db}{\displaybreak[0]}
\newcommand{\nn}{\nonumber}
\newcommand{\ls}{\lesssim}

\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\de}{\delta}
\newcommand{\De}{\Delta}
\newcommand{\ep}{\varepsilon}
\newcommand{\ga}{\gamma}
\newcommand{\Ga}{\Gamma}
\newcommand{\la}{\lambda}
\newcommand{\La}{\Lambda}
\newcommand{\na}{\nabla}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}
\newcommand{\pa}{\partial}
\newcommand{\pr}{\prime}
\newcommand{\si}{\sigma}
\newcommand{\ta}{\theta}
\newcommand{\vp}{\varphi}
\newcommand{\ze}{\zeta}

\newcommand{\dx}{\,\mathrm{d} x}
\newcommand{\ds}{\,\mathrm{d} s}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\a}{\mathfrak{a}}
\renewcommand{\b}{\mathfrak{b}}
\renewcommand{\c}{\mathfrak{c}}
\renewcommand{\d}{\mathfrak{d}}
\newcommand{\e}{\mathfrak{e}}
\newcommand{\g}{\mathfrak{g}}
\renewcommand{\i}{{\rm\mathbf i}}
\newcommand{\ue}{u_\mathcal{E}}
\newcommand{\ua}{u_\mathcal{A}}

\DeclareMathOperator{\re}{{Re}}
\DeclareMathOperator{\im}{{Im}}
\DeclareMathOperator{\ios}{{\mathcal{I}_{\rm Os}}}

\newcommand{\p}{\partial}
\newcommand{\T}{\mathcal{T}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\hl}{\underline{h}}
\newcommand{\gal}{\underline{\gamma}}
\newcommand{\uhfem}{u_h^{\rm FEM}}

\newcommand{\eq}[1]{\begin{align}#1\end{align}}
\newcommand{\eqn}[1]{\begin{align*}#1\end{align*}}


\date{}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 封面 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}

\centerline{\includegraphics[width=76mm,height=19mm]{nju.eps}}

\vskip 1cm

\centerline{\zihao{0} \ziju{0.4} \biaosong 研究生毕业论文}

\bigskip

\centerline{\zihao{2} \ziju{0.3} \kaishu （申请硕士学位）}

%\vspace{1cm}

\vspace{1cm}

\centerline{\includegraphics[width=50mm,height=58mm]{nju.eps}}

{\zihao{4}
\begin{tabbing}

\hspace{1.6cm} \= \hspace{2.8cm} \= \kill

\>{\zihao{3} \kaishu 论文题目: }\>

\underline{\bf\quad 人工神经网络的范畴解释及其应用 \quad\quad\;\,}\\

\>{\zihao{3} \kaishu 作 \hskip 11.5mm  者:} \> \underline{\bf\quad\quad\quad\quad\quad\quad\quad\quad 武伟\quad\quad\quad\quad\quad\quad\quad\quad\,}\\

\>{\zihao{3} \kaishu 专 \hskip 11.5mm  业:} \> \underline{\bf\quad\quad\quad\quad\quad\,\,\,\qquad 计算数学\,\,\,\,\quad\quad\quad\quad\quad}\\

\>{\zihao{3} \kaishu 研究方向: }\> \underline{\bf\quad\quad\quad\quad\quad\quad\quad\,\,\, 范畴理论及其应用\,\,\,\quad\quad\quad\quad\quad\quad\quad} \\

\>{\zihao{3} \kaishu 指导教师: }\> \underline{{\bf \quad\quad\quad\quad\quad\quad\quad \,\,丁南庆\,教授\quad\,\,\quad\quad\quad\quad\quad\,}} \\
\end{tabbing}

%\vspace{1.8cm}

\centerline{\ziju{0.2}
二\hspace{0.1cm}\!\raisebox{0.4mm}{$\bigcirc$\!
一}\!\hspace{0.1cm}六年五月一日} } \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 封二 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}

\vbox{\vskip 18.8cm}

\noindent {\zihao{3} \kaishu 学 \hskip 22.7mm 号：{\Large \tt
\hskip 2mm \qquad MG1321022}

\noindent {\zihao{3} \kaishu 论文答辩日期：\hskip 25mm 年 \hskip
12mm 月 \hskip 12mm 日}

\noindent {\zihao{3} \kaishu 指 \hskip 3.9mm 导 \hskip 3.9mm 教
\hskip 3.9mm 师：\hskip 48mm （签字）}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%% 目录 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \setcounter{page}{1}
 \pagenumbering{Roman}
\renewcommand{\baselinestretch}{1.9}

\small\normalsize

\tableofcontents

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 中文摘要 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.9}
\small\normalsize

\thispagestyle{fancy} \fancyhead[C]{{\kaishu 中文摘要}}

\setcounter{page}{1} \pagenumbering{arabic}

\clearpage \addcontentsline{toc}{section}{\bf 中文摘要 }

\noindent 毕业论文题目： \underline {人工神经网络的范畴解释及其应用}

\noindent \underline {计算数学} 专业\hskip 0.2cm
\underline{2013} 级 硕士生姓名： \underline {武伟}

\noindent 指导教师（姓名、职称）\,：\underline {丁南庆教授}

\vskip 20mm

本文主要是人工神经网络的范畴解释及其应用......

\bigskip

\noindent {\bf 关键词：}\
人工神经网络，范畴，MLP，CNN，ART1


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 英文文摘 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.5} \small\normalsize

\thispagestyle{fancy} \fancyhead[C]{{\kaishu 英文摘要}}

\clearpage \addcontentsline{toc}{section}{\bf 英文摘要 }

\noindent {\bf THESIS:}  \\  the categorical explanation and application of artificial neural networks

\noindent {\bf SPECIALIZATION:} computational mathematics

\noindent {\bf POSTGRADUATE:} wei Wu

\noindent {\bf MENTOR:} Professor nanqing Ding

\vskip10mm

In this thesis, we apply category theory to explain how can neural networks store information......

\medskip

\noindent {\bf Key Words:} artificial neural network，category，MLP，CNN，ART1
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 论文正文 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \renewcommand{\baselinestretch}{1.5}
 \newpage

\clearpage
\addcontentsline{toc}{section}{人工神经网络的范畴解释及其应用}


\clearpage
 \pagestyle{fancy}
 \fancyhead[LR]{}
  \fancyhead[C]{\kaishu{\rightmark}}


\centerline{\Large \heiti 人工神经网络的范畴解释及其应用} \vskip
20mm


\section{人工神经网络}
\subsection{神经网络与人工神经网络}
生物神经网络主要由神经元为主要单元，神经元是大脑处理信息的重要部分。一个神经元的结构如图\ref{fig-sample}所示，其最主要的部分为末梢作为输入和输出的树突和用于传输信号的轴突。当事件发生时信息以电信号的形式在树突上扩布并被整合，电信号在轴突处加以累计，当超过一定的阈值时突破轴突进入作为输出的树突。数十亿个神经元连接成的复杂网络构成了大脑的神经系统。

\begin{figure}[h!]
    \centering
    \includegraphics[width=76mm,height=39mm]{figure1.eps}
    \caption{}
    \label{fig-sample}
\end{figure}

利用计算机可以模拟这样的神经结构，人工神经网络也由人工神经元（下面简称为神经元）构成，其中最基本的结构如图\ref{fig-sample}所示，以神经元m为研究对象，其中$n_k(k=1,2,3)$ 为输入端相连的神经元，每个提供的电流为$a_k(k=1,2,3)$,邻接边上有权重参数$w_k(k=1,2,3)$，在所有的输入之后有激活函数$\sigma(\cdot)$, 则其输出的生物电流为
\[b=\sigma(\sum_{k=1}^{3}a_kw_k)\]
其中的$\sigma(x)$常取作取值在(0,1)之间的sigmoid函数
\[\sigma(x)=\frac{1}{1+e^{-x}}\]
这样的一个结构叫做感知机，一个人工神经网络通常由若干的感知机构成不同的复杂结构。
\subsection{几种典型的神经网络}
人工神经网络的根据不同的任务目标所设计的网络结构种类繁多，下面只介绍本文所需要的三种典型人工神经网络
\subsubsection{自适应共振网络-ART}
\cite{apt}是人脑的自适应共振理论的基础工作，它揭示了人脑神经元的一种简化的组织形式。这种神经元模型具有长期记忆和新颖物体学习的平衡性，并且能够
在线更新知识体系。ART1是自适应共振理论对应的最简单的网络结构，如图\ref{art1-sample}所示
\begin{figure}[h!]
    \centering
    \includegraphics[width=76mm,height=39mm]{art1.eps}
    \caption{}
    \label{art1-sample}
\end{figure}

$F_1$称为输入层，$F_2$称为隐藏层。连接$x_i$和$y_j$的上行和下行权重分别为$z_{ij}^{bu}$和$z_{ij}^{td}$，它们主要负责存储中期记忆,不同之处在于$z_{ij}^{bu}$取[0,1] 之间的实值而$z_{ij}^{td}$只在0和1两个值中取。当$F_1$ 层的0、1二值激活为$I_i(i=1,2,...,N)$ 时，$F_2$ 层的预激活量为\[T_j=\sum_{i=1}^{N}z_{ij}^{bu}I_i, j=1,2,...M\]
$F_2$层的激活向量为
\[\begin{cases}
y_J=1 \qquad if \qquad T_j=max_j{T_j}\\
y_{j\neq J}=0
\end{cases}\]
$F_2$层对$F_1$层的回馈量为
\[V_i=\sum_{j=1}^{M}z_{ji}^{td}y_j=z_{Ji}^{td},\qquad i=1,2,...N\]
门限系统用于控制系统的重启，它首先计算$F_1$层的匹配向量
\[x_i=V_iI_i \qquad or \qquad X=V \bigcap I=z_{J}^{td} \bigcap I\]
如果$\rho |I| \le |I \bigcap z_{J}^{td}|$则从$y_j(j=1,2,...,M)$中重新选择合适的代表元。在上式中$\rho$是预先确定的[0,1]之间的阈值，范数取$l_1$ 范数。整个过程对应的流程图如下图\ref{art1-flow}，其中不加解释地包括了参数$z_{J}^{td}$和$z_{J}^{bu}$的更新步骤。
\begin{figure}[h!]
    \centering
    \includegraphics[width=80mm,height=80mm]{art1-flow.eps}
    \caption{}
    \label{art1-flow}
\end{figure}
ART1网络常用于01序列化特征的聚类。其扩展的网络Fuzzy ART1可用于分类的任务。

\subsubsection{多层感知机-MLP}
多层感知机是感知机的多层复合，以下图\ref{mlp}的3-4-3 MLP为例，其中$x_i\,,y_j\,,z_k$分别是三层的节点，$w_{ij}^{(1)}\,,w_{ij}^{(2)}\,,b^{(1)}\,,b^{(2)}$ 分别是第一二层和第二三层之间的权重参数。
\begin{figure}[h!]
    \centering
    \includegraphics[width=80mm,height=40mm]{mlp.eps}
    \caption{}
    \label{mlp}
\end{figure}
其前向传播过程可表示为
\[\begin{cases}
y_j=\sigma (\sum_{i=1}^{3}w_{ij}^{(1)}x_i+b^{(1)}) \qquad j=1,2,3,4\\
z_k=\sigma (\sum_{j=1}^{2}w_{ij}^{(2)}y_j+b^{(2)}) \qquad k=1,2,3
\end{cases}\]
其中的$\sigma$为sigmoid函数。其参数的训练过程通常采用BP 算法。MLP网络的优势在于网络收敛快速并且可以很容易拓展为深度的网络，其缺点是
网络的参数过多很容易造成数据的过拟合。

\subsubsection{卷积神经网络-CNN}
在机器视觉领域，一个应用更广泛的网络结构称为卷积神经网络。它由一些卷积层和降采样层组合而成。
\begin{figure}[h!]
    \centering
    \includegraphics[width=80mm,height=40mm]{lenet.eps}
    \caption{}
    \label{lenet}
\end{figure}

卷积（Convolution）层由每一层的特征图和带参数的卷积核做二维卷积构成。记其中的特征图是w $\times$ h尺寸的$X_{w,h}$，卷积核是m $\times$ n尺寸的$K_{m,n}$。$X(w_1:w_2,h_1:h_2)$是$X_{w,h}$的子矩阵当$1 \le w_1 \le w_2 \le w$ 且$1 \le h_1 \le h_2 \le h$。当$X_{m,n}$是m $\times$ n的特征图时二维卷积可表示为
\[X_{m,n} \ast K_{m,n} = \sum_{i=1}^{m}\sum_{j=1}^n x_{ij}k_{m-i,n-j} \]
更一般的情况下当$X_{w,h}$是w $\times$ h的特征图时二维卷积可表示为
\[X_{w,h} \ast K_{m,n} =
\left(
  \begin{array}{ccc}
    X(1:m,1:n)     \ast K &...& X(w-m+1:m,1:n)     \ast K\\
    %X(1:m,h-n:h-1) \ast K & X(2:m+1,h-n:h-1) \ast K & X(w-m+1:m,h-n:h-1) \ast K\\
    ...                   &...& ...       \\
    X(1:m,h-n+1:h) \ast K &...& X(w-m+1:m,h-n+1:h) \ast K
  \end{array}
\right)
\]
降采样（pooling）层在特征图的某一个子矩阵中取平均值或最大值。

一个典型的CNN网络是LeNet5，其网络结构如下图\ref{lenet} 所示。可以看出CNN 实际上是MLP网络的一种特例，区别在于：1、CNN把MLP中每一层一维的特征向量重排成特征图。2、 和MLP 中每一层的每一个节点都和相邻层的每个节点存在连接不同，CNN中每层的节点只和相邻层中某个领域中的节点存在连接。CNN网络的结构决定了它对图像的处理具有先天的优势，事实上在多个图像分类检测任务中CNN 网络已经超过了人眼的正确率。

\subsection{人工神经网络的可靠性解释}
长久以来人工神经网络都缺乏一个很好的理论解释，它被看做“黑盒”分类器或特征提取工具在工程领域得到了广泛的应用，却缺乏一个好的理论解释。一种普遍的观点是从数值计算的角度考虑。设A 是一个人工神经网络的结构，w 是其中的一些权重参数，$\theta$ 是另外一些超参数，则神经网络就是一个由$(A,w,\theta)$构成的高度非线性的系统，其中的参数个数巨大，在定义了损失函数$L(A,w,\theta)$，可以将神经网络的训练问题归结于固定网络结构A下的一个非凸优化问题
\begin{displaymath}C = \arg\max_{w,\theta}(L(A,w,theta))
\end{displaymath}
通常使用最速下降等一阶算法求解，这个非凸优化如何得到一个可靠的非局部最优解的问题解决得益于以下两个方面：1、近几年GPU芯片性能的飞跃和计算机存储数据的累积带来数据量的提升，使得最速下降法求解时可以采用一个非常小的步长。2、 一些由物理学启发的学习率的设定技巧带来了一定的收敛性的可靠结果。

从数值计算的角度，可以说解决了固定网络结构A下的人工神经网络的训练问题。但是，仍然存在几个重要的问题：1、人工神经网络为什么会有超越一般算法的效果，能否从数学的角度解释它的知识存储和传递。2、如何更好地设计神经网络以达到更好地性能，能否从数学的角度给予解决方案。

在这里我们将利用范畴论的工具来将知识体系范畴和神经网络范畴统一起来，给出人工神经网络一个范畴论的解释。另外，将从范畴论的角度指导人工神经网络的改造。最后，通过几个matlab的仿真实验来证明范畴解释方法的可靠性以及改进后的神经网络的优越性。M.J.Healy在\cite{h1}\cite{h2}中做了很多相关的工作，这里会在他的基础上：1、完善人工神经网络范畴解释构造的过程和一些省去的证明。2、增加一些新的理论内容。3、对CNN和MLP进行范畴论解释下的对比分析。4、 增加几个有说服力的仿真实验。

\section{人工神经网络中范畴的构建}
在这个部分，首先会构造出知识范畴$\bm{Know}$和神经网络范畴$\bm{Neur}$。 随后为了解释神经网络对知识的存储的表达能力，定义了这两个范畴之间的函子。最后借用范畴中余极限的概念给出人工神经网络层次结构的理论基础。
\subsection{知识范畴$\bm{Know}$}
\begin{definition}
一个符号系统由下面几部分组成：

1.\quad 一个符号的集合S

2.\quad 运算的三元组$O=<C,F,P>$，其中

  C是常数符号的集合

  F是函数的集合

  P是预测符号的集合（一般为布尔值）
\end{definition}
\begin{definition}
一个符号系统映射从一个符号系统映到另一个符号系统，把符号集合映到符号集合，把运算映到运算。
\end{definition}
\begin{definition}
以符号系统为对象，以符号系统映射作为态射构成符号系统范畴$\bm{Sign}$，其中态射的合成是两个映射的合成。
\end{definition}
\begin{definition}
一个声明由以下两部分组成：

1.\quad 一个符号系统$Sig=<S,O>$

2.\quad 一个符号系统$Sig$上的公理集合$Ax$

\end{definition}
\begin{definition}
给定两个声明$<Sig1,Ax1>$和$<Sig2,Ax2>$，则两个符号系统$Sig1$和$Sig2$之间的映射称为声明映射当且仅当：

\[\forall a \in Ax1,(Ax2 \vdash M(a))\]

即每一个$Ax1$中的公理，通过M的翻译之后都可以证明服从$Ax2$中的公理，可以看出$Ax2$是更强的公理。
\end{definition}
\begin{definition}
以声明为对象，以声明映射作为态射构成知识范畴$\bm{Know}$，其中态射的合成是两个声明映射的合成。
\end{definition}
\begin{example}
以知识范畴中的一个几何对象为例，知识$K_1$的定义如下：

Know $K_1$

\qquad sorts Points,Lines

\qquad const p1:Points

\qquad const p2:Points

\qquad const p3:Points

\qquad op on: Points*Lines $->$ Boolean

\qquad Axiom Two-points-define-a-line is

\qquad \qquad for all(x,y:Points)((x not= y)implies

\qquad \qquad \qquad (exist l:Lines) (on(x,l) and on(y,l) and

\qquad \qquad \qquad ((forall m:lines) (on(x,m) and on(y,m)) implies (m=l) )))

上述对象$K_1$中包含了一个符号系统，其中的符号集合包含Points和Lines两个集合，运算包括常数符号p1,p2和p3，算子on是一个预测符号。由符号系统和公理两点确定一条直线
表示了这个特定的知识范畴中的对象$K_1$。

另一个更强的知识对象$K_2$如下：

Know $K_2$

\qquad sorts Points,Lines

\qquad const pa1:Points

\qquad const pa2:Points

\qquad const paext:Points

\qquad const la:Lines

\qquad op on: Points*Lines $->$ Boolean

\qquad Axiom Two-points-define-a-line is

\qquad \qquad for all(x,y:Points)((x not= y)implies

\qquad \qquad \qquad (exist l:Lines) (on(x,l) and on(y,l) and

\qquad \qquad \qquad ((forall m:lines) (on(x,m) and on(y,m)) implies (m=l) )))

\qquad on(pa1,la) and on(pa2,la) and (pa1 not= pa2)

上述对象$K_2$中包含了一个符号系统，其中的符号集合包含Points和Lines两个集合，运算包括常数符号pa1,pa2,paext和la，算子on是一个预测符号。由符号系统和公理两点确定一条直线（特别地，pa1和pa2共线）构成。
$K_2$和$K_1$的不同之处在于常数的命名不同，多了一个线的常数以及多了一些公理的描述。存在知识范畴下的态射$s_1:K_1 \to K_2$如下：

Morphism $s_1$:

\qquad Points $\to$ Points

\qquad Lines $\to$ Lines

\qquad on $\to$ on

\qquad p1 $\to$ pa1

\qquad p2 $\to$ pa2

\qquad p3 $\to$ paext

这样给出了从较弱的对象$K_1$到较强的对象$K_2$的一个态射。事实上，$K_1$中的三个点均为随机游走的点，$K_2$要求其中的两个点共线，第三个点是随机游走的点，显然$K_2$是较强的对象。
\end{example}
\subsection{神经网络范畴$\bm{Neur}$}
无论是人工神经网络还是生物神经网络，神经元都是其组成的最基本的单元。以往的研究通常集中在若干神经元激活值的分布所代表了某些特定的模式，在本文中将重点探讨单个神经元所代表的模式，以及其中蕴含的知识信息。人工神经网络的结构千差万别，但是具体到每个神经元，最基本的结构是：
\[\xi=\sigma(\sum_{I}w_ia_i+b_i)\]
其中$\sigma$是非线性激活函数，I是前层与当前层有连接的指标集，$w_i$和$b_i$ 分别为权重项和偏置项，$x_i$是前层的激活值。
为了定义的方便，我们考虑构造在固定网络结构A和网络权重w固定的情况下的神经网络范畴$N_A,w$，以下会不加说明的记为$\bm{Neur}$。
\begin{definition}
设$p_i$是神经网络中的一个神经元节点，$\eta$是$p_i$激活量$\xi_i$的一个约束数值区间。则所有的$(p_i,\eta)$构成$\bm{Neur}$中的对象。
\end{definition}
显然，在这样的定义下，每个神经元节点反应了在某个特定感受野中激活达到某一区间的抽象知识。为了构造一个完整的范畴，下面来定义神经网络范畴中的态射。
\begin{definition}
设$(p_i,\eta)$和$(p_j,\eta^\prime)$是两个不同的神经元节点。如果存在$c_1,c_2,...,c_n$是全不为零的通路，所通过的节点为$(p_1,p_2,...,p_n)$，满足$p_i=p_1$和$p_j=p_n$，则称其为$(p_i,\eta)$和$(p_j,\eta^\prime)$间的一条信道，记为
\[\gamma=[(p_1,\eta_{1}),c_1,(p_2,\eta_{2}),c_2,...,(p_n,\eta_{n})]\]
所有信道$\gamma$构成的集合记为$\Gamma$，起点和终点分别记为$(p_i,\eta)=\bm{O_s}(\Gamma),(p_j,\eta^\prime)=\bm{O_T}(\Gamma)$。
\end{definition}
\begin{definition}
给定信道$\gamma$，当神经网络的输入向量为$U_{\gamma}$时信道$\gamma$的节点分别在区间$\eta_{1},\eta_{2},...,\eta_{n}$内激活，则称$U_{\gamma}$是信道$\gamma$的一个实例。信道集合$\Gamma$的实例为$U_{\Gamma}=\cap U_{\gamma}(\gamma \in \Gamma)$。
\end{definition}
\begin{definition}
若$\Gamma$和$\Gamma^\prime$是起点和终点相同的信道集合，则称$\Gamma$和$\Gamma^\prime$等价，若它们对应的实例$U_{\Gamma}=U_{\Gamma^\prime}$。记为$\Gamma \equiv \Gamma^\prime$。
\end{definition}
\begin{definition}
若$(p_i,\eta)$和$(p_j,\eta^\prime)$为$\bm{Neur}$中的对象，则定义它们之间的态射为$\bar{\Gamma}:(p_i,\eta) \to (p_j,\eta^\prime)$。其中$\bar{\Gamma}$为从$(p_i,\eta)$到$(p_j,\eta^\prime)$的信道集合的等价类。
\end{definition}
\begin{theorem}
按照上述定义所有$(p_i,\eta)$和$(p_j,\eta^\prime)$作为对象，从$(p_i,\eta)$ 到$(p_j,\eta^\prime)$的信道集合的等价类作为态射，确实构成一个范畴$\bm{Neur}$，称为在权重w和网络结构A给定状态下的神经网络范畴。
\end{theorem}
\begin{proof}
这里只给出态射合成的显式表达，验证构成范畴是平凡的。
设态射\[\bar{\Gamma_1}:(p_i,\eta) \to (p_j,\eta^\prime)\]
\[\bar{\Gamma_2}:(p_j,\eta^\prime) \to (p_k,\eta^\prime\prime)\]
并且对应的实例分别为$U_{\Gamma_1}$和$U_{\Gamma_2}$。于是定义实例的合成为$U_{\Gamma_3} = U_{\Gamma_1} \cap U_{\Gamma_2}$，信道的合成定义为级联。
即若
\[\gamma =[(p_1,\eta),c_1,...,(p_m,\eta^\prime)]\in \Gamma_1\]
\[\gamma^\prime = [(p_m, \eta^\prime),c_m,...,(p_n,\eta^{\prime\prime})] \in \Gamma_2\]
\[\gamma;\gamma^\prime=[(p_1,\eta),c_1,...,(p_n,\eta^{\prime\prime})]\]
则\[\Gamma_3 = {\gamma;\gamma^\prime|\gamma \in \Gamma_1, \gamma^\prime \in \Gamma_2}\]，态射的合成为$\Gamma_3$所在的等价类。
\end{proof}
\subsection{知识范畴到人工神经网络范畴的对应}
至此，我们构造了知识范畴$\bm{Know}$和人工神经网络范畴$\bm{Neur}$。为了寻找神经网络里知识的表达方式和存储的层次结构，需要构造一个从知识范畴$\bm{Know}$到人工神经网络范畴$\bm{Neur}$的一个函子。
\begin{definition}
感受野（receptive field）在生物学上是指某个某个视觉细胞所接受到的范围，在人工神经网络里代表隐藏层每个神经元接受信息的像素范围。
\end{definition}
\begin{definition}
在给定神经网络结构A和权重w时，可以把网络看成一个有向图D。若存在神经元$p_i$和$p_j$之间的连接$\pi:\p_i \to \p_j$，则称$p_i$是$p_j$的一个前继。记为$Suc(p_j)$为所有前继的集合，则记$Recp(p_j)=\bigcup_{i=0}^{+\infty}Suc^{i}(p_j)$为$p_j$的感受野集合。
\end{definition}
\begin{definition}
在给定神经网络结构A和权重w时，对于每一个神经元节点$p_i$，其激活值总可以表示为其感受野集合中神经元激活值的非线性函数，抽象地记为$\xi_i=\theta_i(Recp(p_i))$，其中$\xi_i$为神经元节点$p_i$的激活值。
\end{definition}
有了这一系列的准备工作，可以开始定义两个范畴之间的函子，为了说明神经网络中所蕴含的知识我们采取两种方式来定义这个函子:1.采用一种显式的表达方法。2. 采用实验的方法给出每个神经元代表了什么样的图像语义信息。下面先给出一种显式的方式，实验的方式会在下一节中给出。
\begin{definition}
假设$(p_i,\eta)$和$(p_j,\eta^\prime)$是范畴$\bm{Neur}$中的两个对象并且有$\Gamma_1:(p_i,\eta) \to (p_j,\eta^\prime)$，则存在函子$M:\bm{Neur} \to \bm{Know}$使得
\[M(p_i,\eta)=K_i\]
\[M(p_j,\eta)=K_j\]
其中

Know $K_i$
\[\begin{cases}
sorts \quad Neurons \\
const \quad Recp(p_i):Neurons \\
Axiom: \theta_i(Recp(p_i)) \in \eta
\end{cases}\]

Know $K_j$
\[\begin{cases}
sorts \quad Neurons \\
const \quad Recp(p_j):Neurons\\
Axiom: \\
\qquad \theta_i(Recp(p_i)) \in \eta \\
\qquad \theta_j(Recp(p_j)) \in \eta^\prime
\end{cases}\]
\end{definition}

\begin{remark}
上述函子的定义是良好的。
首先，函子M诱导出$M(\Gamma_1): K_i \to K_j$。
事实上，因为$\Gamma_1:(p_i,\eta) \to (p_j,\eta^\prime)$，则取其中任意一条信道\[\gamma =[(p_i,\eta),c_1,...,(p_j,\eta^\prime)]\in \Gamma_1\]，则根据定义有$p_i \in Recp(p_j)$，故$Recp(p_i) \subset Recp(p_j)$。于是存在嵌入映射$\lambda_1:Recp(p_i)\to Recp(p_j)$，易见Axiom之间同样存在嵌入映射，因此M诱导出$M(\Gamma_1): K_i \to K_j$是嵌入映射。其中$K_j$是较强的知识对象。

其次，函子M保持态射的合成。
假设$(p_k,\eta^\prime\prime) \in \bm{Neur}$并且存在$\Gamma_2:(p_j,\eta^\prime) \to (p_k,\eta^\prime\prime)$，则$K_k=M(p_k,\eta^\prime\prime)$为
Know $K_k$
\[\begin{cases}
sorts \quad Neurons \\
const \quad Recp(p_k):Neurons\\
Axiom: \\
\qquad \theta_k(Recp(p_k)) \in \eta^\prime\prime
\end{cases}\]
因此存在嵌入映射$\lambda_1:Recp(p_i)\to Recp(p_j)$和嵌入映射$\lambda_2:Recp(p_j)\to Recp(p_k)$，则嵌入映射$\lambda_3:Recp(p_i)\to Recp(p_k)$是$\lambda_2 \circ \lambda1$。同理Axiom也满足这样的嵌入映射的合成关系。故$M(\Gamma_2 \circ \Gamma_1)=M(\Gamma_2) \circ M(\Gamma_1)$。
综上所述，函子M的定义是良好的。
\end{remark}
\subsection{神经网络中的层表示}
有了上述构造的知识范畴$\bm{Know}$和人工神经网络范畴$\bm{Neur}$，以及它们之间的函子M，我们试图利用范畴理论的知识来解释人工神经网络中知识的存储机制。具体来说是神经网络为什么会设计成层状的结构，以及每层中包含什么样的知识依附关系。
\begin{definition}
若J和C是范畴，且有图$F:J \to C$，则图F上的余锥是指范畴C中的一个对象N和一族态射
\[\psi_x:F(X) \to N\]
使得对任意J中的对象X，取J中任意的一个态射$f:X \to Y$，都有
\[\psi_y \circ F(f) = \psi_x\]
\end{definition}
\begin{definition}
若J和C是范畴，且有图$F:J \to C$，则图F的余极限是一个余锥$(L,\phi)$满足：对图F的任意其他余锥$(N,\psi)$，存在唯一的态射$u:L \to N$使得$u \circ \phi_x =\psi_x$对所有J中的X。
\begin{figure}[h!]
    \centering
    \includegraphics[width=76mm,height=39mm]{colimit.eps}
    \caption{}
    \label{fig-colimit}
\end{figure}
\end{definition}
\begin{remark}
余极限是余锥范畴中的起始元。
\end{remark}
下面首先给出一个知识范畴$\bm{Know}$内余极限的例子。
\begin{example}
范畴中的对象$K_1$和$K_2$如例2.1所示，另外的两个对象$K_3$和$K_4$如下所示

Know $K_3$

\qquad sorts Points,Lines

\qquad const pb1:Points

\qquad const pb2:Points

\qquad const pbext:Points

\qquad const lb:Lines

\qquad op on: Points*Lines $\to$ Boolean

\qquad Axiom Two-points-define-a-line is

\qquad \qquad for all(x,y:Points)((x not= y)implies

\qquad \qquad \qquad (exist l:Lines) (on(x,l) and on(y,l) and

\qquad \qquad \qquad ((forall m:lines) (on(x,m) and on(y,m)) implies (m=l) )))

\qquad on(pb1,lb) and on(pb2,lb) and (pb1 not= pb2)

$K_3$表示$pb1$和$pb2$共线，$pbext$随机游走的知识对象。
同理定义$K_4$表示$pc1$和$pc2$共线，$pcext$随机游走的知识对象。
态射$s_1: K_1 \to K_2$在例2.1中已经定义，$s_2: K_1 \to K_3$的定义如下：

Morphism $s_2$:

\qquad Points $\to$ Points

\qquad Lines $\to$ Lines

\qquad on $\to$ on

\qquad p1 $\to$ pbext

\qquad p2 $\to$ pb1

\qquad p3 $\to$ pb2

态射$s_3: K_1 \to K_4$的定义如下：

Morphism $s_3$:

\qquad Points $\to$ Points

\qquad Lines $\to$ Lines

\qquad on $\to$ on

\qquad p1 $\to$ pc2

\qquad p2 $\to$ pcext

\qquad p3 $\to$ pc1

于是原图$\Delta$可以在余极限的概念下扩张为$\bar{\Delta}$，其中($K_5,l_i$)表示原图$\Delta$的余极限。
$K_5$可以表示为

Know $K_5$

\qquad sorts Points,Lines

\qquad const p1:Points

\qquad const p2:Points

\qquad const p3:Points

\qquad const la:Lines

\qquad const lb:Lines

\qquad const lc:Lines

\qquad op on: Points*Lines $\to$ Boolean

\qquad Axiom Two-points-define-a-line is

\qquad \qquad for all(x,y:Points)((x not= y)implies

\qquad \qquad \qquad (exist l:Lines) (on(x,l) and on(y,l) and

\qquad \qquad \qquad ((forall m:lines) (on(x,m) and on(y,m)) implies (m=l) )))

\qquad on(p1,la) and on(p2,la) and (p1 not= p2)

\qquad on(p2,lb) and on(p3,lb) and (p2 not= p3)

\qquad on(p3,lc) and on(p1,lc) and (p3 not= p1)

态射$l_1:K_1 \to K_5,l_2:K_2 \to K_5,l_3:K_3 \to K_5, l_4:K4 \to K_5$分别是分量对应的嵌入映射。
\end{example}

这表示余极限是由已知不共线的三点构成的三角形。

\begin{figure}[h!]
    \centering
    \includegraphics[width=76mm,height=39mm]{ex_colimit.eps}
    \caption{}
    \label{ex_colimit}
\end{figure}

有了如上的准备工作，下面我们可以具体研究神经网络知识存储的方式：1、神经网络为何组织成层状结构。2、层状结构中后面层中节点与前面层中节点所代表的知识对象有什么样的关系。下面以含有跨层连接的2-2-1MLP网络为例，为了说明的方便需要几点假设：
\begin{hyper}

1、左图是神经网络范畴$Neur$中的交换图。

2、第n+1层的神经元节点$(p_5,\eta_5)$是左图中第n层及第n-1层的神经元节点$(p_i,\eta_i)$的余极限。

3、从神经网络范畴$Neur$到知识范畴$Know$的函子M是保持余极限的函子。
\end{hyper}
\begin{figure}[h!]
    \centering
    \includegraphics[width=76mm,height=39mm]{hyper.eps}
    \caption{}
    \label{hyper}
\end{figure}
\begin{remark}
提出一系列假设的原因是给出神经网络设计的一种可能的隐藏原理，试图通过这些假设推出一系列结论并且通过实验来证明这些结论确实存在，那么被证明的推论越多，神经网络的知识表示越可能是假设中的表达。其中第二点假设是最核心的，即神经网络通过计算余极限实现知识的前向传播。
\end{remark}
假设由第n-1层和第n层构成图$\Delta$，并且有
\[\bar{\Gamma}_{25}= \bar{\Gamma}_{24} \circ \bar{\Gamma}_{45}=\bar{\Gamma}_{23} \circ \bar{\Gamma}_{35}\]
\[\bar{\Gamma}_{15}= \bar{\Gamma}_{14} \circ \bar{\Gamma}_{45}=\bar{\Gamma}_{13} \circ \bar{\Gamma}_{35}\]
其中$\bar{\Gamma}_{ij}$是信道集合的等价类。$((p_5,\eta_5),\bar{\Gamma}_{35},\bar{\Gamma}_{15},\bar{\Gamma}_{25},\bar{\Gamma}_{45})$是图$\Delta$的余极限。
$K_i=M((p_i,\eta_i))$，设$K_1,K_2,K_3,K_4$构成图$\Lambda$。由于M保持余极限，故$(K_5,t_{35},t_{15},t_{25},t_{45})$是图$\Lambda$的余极限。由于知识范畴中的态射是对应sort 和axiom的嵌入，则余极限表示各个知识对象的“组合”（如例2.2所示）。在下一节会通过可视化的方法给出每个神经元所存储的像素信息，并且通过实验验证第n+1层的隐藏层的像素特征信息确实是第n层和第n-1层像素信息的“组合”，这样从侧面证明了$(K_5,t_{35},t_{15},t_{25},t_{45})$是图$\Lambda$的余极限，从而说明了神经网络结构存储知识的一种可能性。

\section{利用范畴理论改进人工神经网络}
\subsection{网络结构的改进}
\subsubsection{改进的FuzzyART网络}
ART1网络广泛地作为二值化向量的聚类网络。在\cite{h2}中Healy改进了ART1网络的结构使得达到了更好地聚类效果。FuzzyART作为ART1网络的一种改进适用于将实值的向量进行聚类，我们首先简单的介绍一个ART1的变形即SIN，由于\cite{h3}证明了在区间的分辨率下SIN和FuzzyART1是等价的，所以我们只需要改进SIN就可以在一个低分辨率的情况下改进FuzzyART1，并且理论上令区间的长度趋于0便可以得到FuzzyART1的改进。
\begin{definition}(SIN)
设区间长为$\delta>0$，区间节点个数为2N（其中N个为补节点），即
\[I_i=(i\delta, +\infty) \qquad i=0,1,...,N-1\]
\[I_i^C=(-\infty,i\delta) \qquad i=N,N+1,...,2N-1\]
则对任意的实值$v \in (-\infty, +\infty)$存在最大的p和最小的q，使得$p\delta < v \le q\delta$，故有映射$\pi: v \to B_{N,\delta}=(b_0,b_1,...,b_{2N-1})$，其中
\[
b_i = 
\begin{cases}
\bm{1}\{v \in I_i\} \qquad i \in 0,1,...,N-1\\
\bm{1}\{v \in I_i^C\} \qquad i \in N,N+1,...,2N-1
\end{cases}
\]
$\bm{1}\{\cdot\}$是布尔函数当条件成立时为1。综上，$\pi$将一个实值映为一个二值的序列。
\end{definition}

\begin{example}
当$\delta=1,N=4$时，
\end{example}

\subsection{多感知器协同工作}

\section{总结与展望}

\clearpage \fancyhead[LR]{} \addcontentsline{toc}{section}{\bf
参考文献}
\renewcommand{\baselinestretch}{1.5}
\small\normalsize



\begin{thebibliography}{99}
\bibitem{apt} Stephen Grossberg, {\em Adaptive Resonance Theory:How a brain learns to consciously attend,learn,and
recognize a changing world}. Neural Networks, {\bf 37}(2013),pp.1-47.

\bibitem{h1} M.J.Healy,Thomas P.Caudell, {\em A categorical semantic analysis of ART architectures}.IJCNN, 2011.

\bibitem{h2} Modification of the ART-1 architecture based on category theoretic design principles.IJCNN,2005.


\bibitem{apt} Stephen Grossberg, {\em Adaptive Resonance Theory:How a brain learns to consciously attend,learn,and
recognize a changing world}. Neural Networks, {\bf 37}(2013),pp.1-47.

\bibitem{dy} Yu Du and Haijun Wu, {\em Preasymptotic error analysis of higher order FEM and CIP-FEM
for Helmholtz equation with high wave number}. SIAM J. Numer. Anal., {\bf 53}(2015), pp.
782C804

\bibitem{bgt} Bayliss, A., Gunzburger, M., and Turkel, E. 1982. {\em Boundary conditionsfor the numerical
solution of elliptic equations in exterior regions.SIAM Journal of Applied Mathematics}
, {\bf 42}: 430C451.

\bibitem{wu}武海军, {\em 偏微分方程数值解法II},pp.61-73

\bibitem{cw} Zhiming Chen and Haijun Wu, {\em Selected Topics in Finite Element Methods}, Science Press
Beijing, 2010.

\bibitem{ls} G.J Lord and A.M Stuart, {\em Discrete Gevrey regularity attractors and upperssemicontinuity for a finite difference approximation to the GinzburgCLandau equation},
Numer. funct. anal. and optim., {\bf 16} (1995), pp. 1003C1047.

\thispagestyle{fancy} \fancyhead[C]{{\kaishu 参考文献 }}

\thispagestyle{fancy} \fancyhead[C]{{\kaishu 参考文献 }}
\end{thebibliography}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 致谢 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.8}
\small\normalsize \thispagestyle{fancy} \fancyhead[C]{{\kaishu
致谢 }} \clearpage \addcontentsline{toc}{section}{\bf 致谢 }

\centerline{\Large\bf 致\quad 谢}

\vskip 10mm

{
\bigskip

经过长时间的努力，终于完成了这篇论文，
在本文的写作过程当中，得到了许多老师和同学的热心帮助，在此向他们表示感谢。

首先要感谢我的导师丁南庆老师，他对学术的专注和对学生的关怀，让我深受感动，也为我将来工作和生活树立了榜样。
丁老师对我的毕业论文的构思、选题都悉心指导，
并且在论文的写作当中，给予了我很多建设性意见，对此我表示衷心的感谢！

同时，我还要感谢三年来给予我帮助的所有老师和同学，他们给我带来了丰富的计算数学的基础知识和深入研究问题的方法，使我受益匪浅。
感谢我的同门师兄妹，大家互助友爱，共同学习，一起营造了一个良好的学术氛围。

最后，感谢我的家人一直以来对我的关心和支持。

}


\end{document}
